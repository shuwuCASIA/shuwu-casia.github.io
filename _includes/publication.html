<h2>2024</h2>
<ul>
<li><a href="找不到">DIVE: Subgraph Disagreement for Graph Out-of-Distribution Generaliztion</a>.<br>Xin Sun, Liang Wang, Qiang Liu, Shu Wu, Zilei Wang, Liang Wang.<i>Knowledge Discovery and Data Mining</i>, 2024. [<a href="javascript:copy(div0, bib0)">bib</a>]<br>
<div id="div0"></div><div id="bib0" style="display:none">
<div class="bib">
<pre>
还没有bib
</pre>
</div>
</div> </li>
<li><a href="https://arxiv.org/abs/2402.14836v1">Stealthy Attack on Large Language Model based Recommendation</a>.<br>Jinghao Zhang, Yuting Liu, Qiang Liu, Shu Wu, Guibing Guo, Liang Wang. <i>Annual Meeting of the Association for Computational Linguistics</i>, 2024. [<a href="javascript:copy(div1, bib1)">bib</a>]<br>
<div id="div1"></div><div id="bib1" style="display:none">
<div class="bib">
<pre>
@article{zhang2024stealthy,
        title={Stealthy Attack on Large Language Model based Recommendation},
        author={Zhang, Jinghao and Liu, Yuting and Liu, Qiang and Wu, Shu and Guo, Guibing and Wang, Liang},
        journal={arXiv preprint arXiv:2402.14836},
        year={2024}
        }
</pre>
</div>
</div> </li>
<li><a href="https://arxiv.org/abs/2402.13593v1">Knowledge Graph Enhanced Large Language Model Editing</a>.<br>Mengqi Zhang, Xiaotian Ye, Qiang Liu, Pengjie Ren, Shu Wu, Zhumin Chen. <i>Annual Meeting of the Association for Computational Linguistics, findings</i>, 2024. [<a href="javascript:copy(div2, bib2)">bib</a>]<br>
<div id="div2"></div><div id="bib2" style="display:none">
<div class="bib">
<pre>
@article{zhang2024knowledge,
        title={Knowledge Graph Enhanced Large Language Model Editing},
        author={Zhang, Mengqi and Ye, Xiaotian and Liu, Qiang and Ren, Pengjie and Wu, Shu and Chen, Zhumin},
        journal={arXiv preprint arXiv:2402.13593},
        year={2024}
        }
</pre>
</div>
</div> </li>
<li><a href="https://arxiv.org/abs/2402.11622v1">Logical Closed Loop: Uncovering Object Hallucinations in Large Vision-Language Models</a>.<br>Junfei Wu, Qiang Liu, Ding Wang, Jinghao Zhang, Shu Wu, Liang Wang, Tieniu Tan. <i>Annual Meeting of the Association for Computational Linguistics, findings</i>, 2024. [<a href="javascript:copy(div3, bib3)">bib</a>]
[<a href="https://github.com/Hyperwjf/LogicCheckGPT">code</a>]
<div id="div3"></div><div id="bib3" style="display:none">
<div class="bib">
<pre>
@article{wu2024logical,
        title={Logical Closed Loop: Uncovering Object Hallucinations in Large Vision-Language Models},
        author={Wu, Junfei and Liu, Qiang and Wang, Ding and Zhang, Jinghao and Wu, Shu and Wang, Liang and Tan, Tieniu},
        journal={arXiv preprint arXiv:2402.11622},
        year={2024}
        }
</pre>
</div>
</div> </li>
<li><a href="https://arxiv.org/abs/2402.14382v1">Enhancing Temporal Knowledge Graph Forecasting with Large Language Models via Chain-of-History Reasoning</a>.<br>Yuwei Xia, Ding Wang, Qiang Liu, Liang Wang, Shu Wu, Xiao-Yu Zhang. <i>Annual Meeting of the Association for Computational Linguistics, findings</i>, 2024. [<a href="javascript:copy(div4, bib4)">bib</a>]<br>
<div id="div4"></div><div id="bib4" style="display:none">
<div class="bib">
<pre>
@article{xia2024enhancing,
        title={Enhancing Temporal Knowledge Graph Forecasting with Large Language Models via Chain-of-History Reasoning},
        author={Xia, Yuwei and Wang, Ding and Liu, Qiang and Wang, Liang and Wu, Shu and Zhang, Xiaoyu},
        journal={arXiv preprint arXiv:2402.14382},
        year={2024}
        }
</pre>
</div>
</div> </li>
<li><a href="https://arxiv.org/abs/2310.09754v2">EX-FEVER: A Dataset for Multi-hop Explainable Fact Verification</a>.<br>Huanhuan Ma, Weizhi Xu, Yifan Wei, Liuji Chen, Liang Wang, Qiang Liu, Shu Wu, Liang Wang. <i>Annual Meeting of the Association for Computational Linguistics, findings</i>, 2024. [<a href="javascript:copy(div5, bib5)">bib</a>]<br>
<div id="div5"></div><div id="bib5" style="display:none">
<div class="bib">
<pre>
@article{ma2023ex,
        title={EX-FEVER: A Dataset for Multi-hop Explainable Fact Verification},
        author={Ma, Huanhuan and Xu, Weizhi and Wei, Yifan and Chen, Liuji and Wang, Liang and Liu, Qiang and Wu, Shu},
        journal={arXiv preprint arXiv:2310.09754},
        year={2023}
        }
</pre>
</div>
</div> </li>
<li><a href="https://www.sciencedirect.com/science/article/abs/pii/S0031320324003807">MetaTKG++: Learning Evolving Factor Enhanced Meta-knowledge for Temporal Knowledge Graph Reasoning</a>.<br>Yuwei Xia, Mengqi Zhang, Qiang Liu, Liang Wang, Shu Wu, Xiaoyu Zhang, Liang Wang. <i>Pattern Recognition</i>, 2024. [<a href="javascript:copy(div6, bib6)">bib</a>]<br>
<div id="div6"></div><div id="bib6" style="display:none">
<div class="bib">
<pre>
@article{xia2024metatkg++,
        title={MetaTKG++: Learning evolving factor enhanced meta-knowledge for temporal knowledge graph reasoning},
        author={Xia, Yuwei and Zhang, Mengqi and Liu, Qiang and Wang, Liang and Wu, Shu and Zhang, Xiaoyu},
        journal={Pattern Recognition},
        pages={110629},
        year={2024},
        publisher={Elsevier}
        }
</pre>
</div>
</div> </li>
<li><a href="https://ieeexplore.ieee.org/abstract/document/10508127">Out-of-distribution Evidence-aware Fake News Detection via Dual Adversarial Debiasing</a>.<br>Qiang Liu, Junfei Wu, Shu Wu, Liang Wang. <i>IEEE Transactions on Knowledge and Data Engineering</i>, 2024. [<a href="javascript:copy(div7, bib7)">bib</a>]<br>
<div id="div7"></div><div id="bib7" style="display:none">
<div class="bib">
<pre>
@article{liu2024out,
        title={Out-of-distribution Evidence-aware Fake News Detection via Dual Adversarial Debiasing},
        author={Liu, Qiang and Wu, Junfei and Wu, Shu and Wang, Liang},
        journal={IEEE Transactions on Knowledge and Data Engineering},
        year={2024},
        publisher={IEEE}
        }
</pre>
</div>
</div> </li>
<li><a href="https://arxiv.org/abs/2105.11866v4">GraphFM: Graph Factorization Machines for Feature Interaction Modeling</a>.<br>Shu Wu, Zekun Li, Yunyue Su, Zeyu Cui, Xiaoyu Zhang and Liang Wang. <i>Machine Intelligence Research</i>, 2024. [<a href="javascript:copy(div8, bib8)">bib</a>]
[<a href="https://github.com/CRIPAC-DIG/GraphCTR">code</a>]<br>
<div id="div8"></div><div id="bib8" style="display:none">
<div class="bib">
<pre>
@misc{wu2024graphfm,
        title={GraphFM: Graph Factorization Machines for Feature Interaction Modeling}, 
        author={Shu Wu and Zekun Li and Yunyue Su and Zeyu Cui and Xiaoyu Zhang and Liang Wang},
        year={2024},
        eprint={2105.11866},
        archivePrefix={arXiv},
        primaryClass={cs.LG}
        }
</pre>
</div>
</div> </li>
<li><a href="https://pubs.acs.org/doi/abs/10.1021/acs.jcim.3c01468">Molecular Contrastive Pretraining with Collaborative Featurizations</a>.<br>Yanqiao Zhu, Dingshuo Chen, Yuanqi Du, Yingze Wang, Qiang Liu, Shu Wu. <i>Journal of Chemical Information and Modeling</i>, 2024. [<a href="javascript:copy(div9, bib9)">bib</a>]<br>
<div id="div9"></div><div id="bib9" style="display:none">
<div class="bib">
<pre>
@article{zhu2024molecular,
        title={Molecular Contrastive Pretraining with Collaborative Featurizations},
        author={Zhu, Yanqiao and Chen, Dingshuo and Du, Yuanqi and Wang, Yingze and Liu, Qiang and Wu, Shu},
        journal={Journal of Chemical Information and Modeling},
        year={2024},
        publisher={ACS Publications}
        }
</pre>
</div>
</div> </li>
<li><a href="https://dl.acm.org/doi/abs/10.1145/3589334.3645478">Semantic Evolvement Enhanced Graph Autoencoder for Rumor Detection</a>.<br>Xiang Tao, Liang Wang, Qiang Liu, Shu Wu, Liang Wang. <i>International World Wide Web Conference</i>, 2024. [<a href="javascript:copy(div10, bib10)">bib</a>]<br>
<div id="div10"></div><div id="bib10" style="display:none">
<div class="bib">
<pre>
@inproceedings{10.1145/3589334.3645478,
        author = {Tao, Xiang and Wang, Liang and Liu, Qiang and Wu, Shu and Wang, Liang},
        title = {Semantic Evolvement Enhanced Graph Autoencoder for Rumor Detection},
        year = {2024},
        isbn = {9798400701719},
        publisher = {Association for Computing Machinery},
        address = {New York, NY, USA},
        url = {https://doi.org/10.1145/3589334.3645478},
        doi = {10.1145/3589334.3645478},
        abstract = {Due to the rapid spread of rumors on social media, rumor detection has become an extremely important challenge. Recently, numerous rumor detection models which utilize textual information and the propagation structure of events have been proposed. However, these methods overlook the importance of semantic evolvement information of event in propagation process, which is often challenging to be truly learned in supervised training paradigms and traditional rumor detection methods. To address this issue, we propose a novel semantic evolvement enhanced Graph Autoencoder for Rumor Detection (GARD) model in this paper. The model learns semantic evolvement information of events by capturing local semantic changes and global semantic evolvement information through specific graph autoencoder and reconstruction strategies. By combining semantic evolvement information and propagation structure information, the model achieves a comprehensive understanding of event propagation and perform accurate and robust detection, while also detecting rumors earlier by capturing semantic evolvement information in the early stages. Moreover, in order to enhance the model's ability to learn the distinct patterns of rumors and non-rumors, we introduce a uniformity regularizer to further improve the model's performance. Experimental results on three public benchmark datasets confirm the superiority of our GARD method over the state-of-the-art approaches in both overall performance and early rumor detection.},
        booktitle = {Proceedings of the ACM on Web Conference 2024},
        pages = {4150–4159},
        numpages = {10},
        keywords = {graph autoencoder, rumor detection, social media},
        location = {<conf-loc>, <city>Singapore</city>, <country>Singapore</country>, </conf-loc>},
        series = {WWW '24}
        }
</pre>
</div>
</div> </li>
<li><a href="https://epubs.siam.org/doi/abs/10.1137/1.9781611978032.53">CAMLO: Cross-Attentive Multi-View Network for Long-Term Origin-Destination Flow Prediction</a>.<br>Liang Wang, Hao Fu, Shu Wu, Qiang Liu, Xuelei Tan, Fangsheng Huang, Mengdi Zhang, Wei Wu. <i>SIAM International Conference on Data Mining</i>, 2024. [<a href="javascript:copy(div11, bib11)">bib</a>]<br>
<div id="div11"></div><div id="bib11" style="display:none">
<div class="bib">
<pre>
@inbook{doi:10.1137/1.9781611978032.53,
        author = {Liang Wang and Hao Fu and Shu Wu and Qiang Liu and Xuelei Tan and Fangsheng Huang and Mengdi Zhang and Wei Wu},
        title = {CAMLO: Cross-Attentive Multi-View Network for Long-Term Origin-Destination Flow Prediction},
        booktitle = {Proceedings of the 2024 SIAM International Conference on Data Mining (SDM)},
        chapter = {},
        pages = {454-462},
        doi = {10.1137/1.9781611978032.53},
        URL = {https://epubs.siam.org/doi/abs/10.1137/1.9781611978032.53},
        eprint = {https://epubs.siam.org/doi/pdf/10.1137/1.9781611978032.53},
                abstract = { Predicting the volume of flow from an origin to a destination is essential to understanding the mobility pattern and improving many transportation services, such as ride-hailing and food delivery. However, making long-term prediction for all possible pairs of origins and destinations is still a challenging problem. Existing works either suffer from cumulative error and high complexity of iterative computation on long sequence, or neglect the complex spatial correlation obscured by elongated time span. In this paper, we present CAMLO, a cross-attentive multi-view network for this task. Our model adopts a multi-view framework that separately models the asymmetric characteristics of origin and destination in origin- and destination-oriented view. In each view, a relational graph aggregation module captures the sparse and multi-relational correlation among origins and destinations. Subsequently, a Transformer-based forecasting module is applied to discover intricate temporal dynamics. The two views internally interact with each other via a cross-attention mechanism and are later fused for the final prediction. Extensive experiments conducted on two real-world datasets demonstrate the effectiveness of our model. }
        }          
</pre>
</div>
</div> </li>

</ul>
<h2>2023</h2>
<ul>

</ul>
<h2>2022</h2>
<ul>

</ul>


<h2>2021</h2>
<ul>

</ul>


<h2>2020</h2>
<ul>

</ul>


<h2>2019</h2>
<ul>

</ul>


<h2>2018</h2>
<ul>

</ul>


<h2>2017</h2>
<ul>

</ul>

<h2>2016</h2>
<ul>

</ul>


<h2>2015</h2>
<ul>

</ul>


<h2>2014</h2>
<ul>

</ul>


<h2>2013</h2>
<ul>

</ul>


<h2>2012</h2>
<ul>

</ul>


<h2>2011</h2>
<ul>

</ul>


<h2>2010</h2>
<ul>

</ul>